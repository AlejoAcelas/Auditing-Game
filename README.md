# Auditing-Game
An exploration of whether [mechanistic interpretability](https://transformer-circuits.pub/2022/mech-interp-essay/index.html) techniques can be used to identify backdoors on Transformer models trained on simple algorithmic tasks. This work was developed as part of the Supervised Program for Alignment Research under the guidance of Marius Hobbhahn.

# Documents in progress
- Finding backdoors in a 1-layer transformer trained to predict the maximum of a sequence ([Colab](https://colab.research.google.com/drive/16eXWffxc1TYG6d2akePofsdDI-Dv6Mzi?usp=sharing))
- Finding backdoors in a 6-layer transformer trained on 5-digit addition ([Docs](https://docs.google.com/document/d/1RFaVCxfaxh4DRRqUmb8BMDItBu7RMVNtwoHZ5qHxIk8/edit?usp=share_link), [Colab](https://colab.research.google.com/drive/1dOt5I9BmKRJ1Y0Zc_ZP6NfAvbtqH-NMm?usp=sharing))
